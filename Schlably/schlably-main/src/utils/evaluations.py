"""
This file provides the EvaluationHandler class, which supports the evaluation of a model testing.
"""
import numpy as np
import time



class EvaluationHandler:
    """
    This class keeps track of several environment parameters (e.g. makespan, tardiness) during a model testing.
    After a testing, you can call the evaluate_test function
    to compute evaluation metrics across all collected test episodes (e.g. mean, standard deviation).

    You can adapt evaluate_test to compute different or more metrics.
    """
    def __init__(self):
        #  test parameters. Valid for a complete test loop by one agent. Including multiple episodes
        self.rewards = []
        self.tardiness = []
        self.tardiness_max = []
        self.makespan = []  # number of steps required to end all jobs
        self.actions_list = []
        self.tasks_list = []
        self.start_time = None


    def record_environment_episode(self, env, total_reward) -> None:
        """
        Stores all necessary environment parameters from the recent episode

        :param env: Non reset environment object, whose parameters should be recorded
        :param total_reward: Total reward of the episode

        :return: None

        """
        """
        if self.start_time is None:
            self.start_time = time.time()
            
        elapsed_time = time.time() - self.start_time
        print(f"Elapsed Time for Episode: {elapsed_time} seconds")
        """
        
        
        # append data from test run to list
        self.makespan.append(env.get_makespan())
        self.rewards.append(total_reward)
        self.tardiness.append(sum(env.tardiness))
        self.tardiness_max.append(max(env.tardiness))
        self.tasks_list.append(env.tasks)
        self.actions_list.append(env.action_history)

    def update_episode_solved_with_solver(self, env) -> None:
        """
        Calculates all missing parameters of an environment processed by the solver

        :param env: Environment object with task attribute generated by the solver

        :return: None

        """
        # calculate tardiness
        env.calculate_tardiness()
        # prepare get_makespan function call
        for task in env.tasks:
            if task.finished > env.ends_of_machine_occupancies[task.selected_machine]:
                env.ends_of_machine_occupancies[task.selected_machine] = task.finished

        self.record_environment_episode(env, 0)

    def evaluate_test(self) -> dict:
        """
        Gets all test_parameter and computes all relevant statistical data for plots and prints

        :return: Dictionary with all specified evaluation metrics

        """
        
        """
        data_path = "C:\\Users\\wlsdm\\Desktop\\진은서\\대학교\\4학년\\시스템종합설계\\schlably-main\\result_dqn_j15_500_3.txt"
 
        with open(data_path,'w') as file:
            # makespan값과 tardiness값을 str형태로 저장한 list구현

            list_makespan = []
            for i in self.makespan:
                list_makespan.append(str(i))
                
            list_tardiness = []
            for i in self.tardiness:
                list_tardiness.append(str(i))                 
                
            list_all = []
            for i in range(len(self.makespan)):
                arr = []
                dat_n = 100
                arr.append(dat_n+i)
                arr.append(list_makespan[i])
                arr.append(list_tardiness[i])
                list_all.append(arr)

            for i in list_all:
                file.writelines(["data: ",str(i[0]), "\n makespan = ",str(i[1]) , "\n tardiness = ", str(i[2]),"\n\n"])

        file.close
        """
        
        
        rewards, tardiness = np.asarray(self.rewards), self.tardiness
        evaluation_results = {}
        evaluation_results['rew_mean'] = np.mean(rewards)
        evaluation_results['rew_std'] = np.std(rewards)
        evaluation_results['rew_best'] = np.max(rewards)
        evaluation_results['rew_best_count'] = sum([1 for el in rewards if el==evaluation_results['rew_best']])
        evaluation_results['rew_worst'] = np.min(rewards)
        evaluation_results['tardiness'] = self.tardiness
        evaluation_results['makespan'] = self.makespan
        evaluation_results['tardiness_mean'] = np.mean(tardiness)
        evaluation_results['tardiness_std'] = np.std(tardiness)
        evaluation_results['tardiness_max_mean'] = np.mean(self.tardiness_max)
        evaluation_results['makespan_mean'] = np.mean(self.makespan)
        evaluation_results['rew_worst_quantile_border'] = np.quantile(rewards, 0.1)
        evaluation_results['rew_cvar'] = rewards[rewards <= evaluation_results['rew_worst_quantile_border']].mean()
        evaluation_results['rew_perc_good_solutions'] = 1 - np.count_nonzero(rewards)/len(rewards)
        evaluation_results['num_tests'] = len(rewards)

        return evaluation_results

    @classmethod
    def add_solver_gap_to_results(cls, results: dict) -> dict:
        """
        If solver makespan exists, compute optimal gap for all agents

        :param results: Dictionary with test results

        :return: Updated dictionary with test results now including optimal gap

        """
        if 'solver' in results:
            optimal_makespan = results['solver']['makespan_mean']
            for agent, result in results.items():
                gap = result['makespan_mean'] - optimal_makespan
                results[agent].update({'gap_to_solver': gap})
        return results
